{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4794e4e3",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d910445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import nltk                               \n",
    "import matplotlib.pyplot as plt           \n",
    "import random  \n",
    "\n",
    "import re                                \n",
    "import string                            \n",
    "\n",
    "from nltk.corpus import stopwords          \n",
    "from nltk.stem import PorterStemmer       \n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421f4d69",
   "metadata": {},
   "source": [
    "## Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de9e5dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('C:/Users/ZASHANK/Downloads/nlp-getting-started/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a585a2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('C:/Users/ZASHANK/Downloads/nlp-getting-started/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "462f8907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "    id keyword location                                               text  \\\n",
      "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
      "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
      "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
      "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
      "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
      "\n",
      "   target  \n",
      "0       1  \n",
      "1       1  \n",
      "2       1  \n",
      "3       1  \n",
      "4       1  \n",
      "====================================================================================================\n",
      "Test Data:\n",
      "    id keyword location                                               text\n",
      "0   0     NaN      NaN                 Just happened a terrible car crash\n",
      "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
      "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
      "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
      "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan\n"
     ]
    }
   ],
   "source": [
    "# Diplaying top 5 records of train and test data\n",
    "print('Train Data:\\n',df_train.head(5))\n",
    "print(100*'=')\n",
    "print('Test Data:\\n',df_test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0379480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape:\n",
      " (7613, 5)\n",
      "====================================================================================================\n",
      "Test Shape:\n",
      " (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "# Shape of Train and Test data\n",
    "print('Train Shape:\\n',df_train.shape)\n",
    "print(100*'=')\n",
    "print('Test Shape:\\n',df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f932b24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        7613 non-null   int64 \n",
      " 1   keyword   7552 non-null   object\n",
      " 2   location  5080 non-null   object\n",
      " 3   text      7613 non-null   object\n",
      " 4   target    7613 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 297.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# Getting info\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e21d1c92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4342\n",
       "1    3271\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "014d771e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting a Pie Chart for the disaster and non-disaster tweets\n",
    "fig,plot = plt.subplots(figsize = (8,4))\n",
    "labels = ['Disaster','Non-Disaster']\n",
    "plot.pie((df_train[df_train['target']==1].shape[0],df_train[df_train['target']==0].shape[0]),labels = labels,autopct='%1.1f%%',\n",
    "        shadow=False, startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f27b7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       our deeds are the reason of this #earthquake m...\n",
       "1                  forest fire near la ronge sask. canada\n",
       "2       all residents asked to 'shelter in place' are ...\n",
       "3       13,000 people receive #wildfires evacuation or...\n",
       "4       just got sent this photo from ruby #alaska as ...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding a bridge collapse int...\n",
       "7609    @aria_ahrary @thetawniest the out of control w...\n",
       "7610    m1.94 [01:04 utc]?5km s of volcano hawaii. htt...\n",
       "7611    police investigating after an e-bike collided ...\n",
       "7612    the latest: more homes razed by northern calif...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting all the string to the lower case letter\n",
    "#df_train.str.lower()\n",
    "df_train['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7baeb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      just happened a terrible car crash\n",
       "1       heard about #earthquake is different cities, s...\n",
       "2       there is a forest fire at spot pond, geese are...\n",
       "3                apocalypse lighting. #spokane #wildfires\n",
       "4           typhoon soudelor kills 28 in china and taiwan\n",
       "                              ...                        \n",
       "3258    earthquake safety los angeles ûò safety faste...\n",
       "3259    storm in ri worse than last hurricane. my city...\n",
       "3260    green line derailment in chicago http://t.co/u...\n",
       "3261    meg issues hazardous weather outlook (hwo) htt...\n",
       "3262    #cityofcalgary has activated its municipal eme...\n",
       "Name: text, Length: 3263, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dad4078",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8942aefb",
   "metadata": {},
   "source": [
    "### Removing punctuations\n",
    "If we did not remove punctuation then punctuation is also considered one word for this situation we remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1157efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string,time\n",
    "exclude = string.punctuation\n",
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char,'')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c1648b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "089726fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train.loc[:,'text'].apply(lambda x : remove_punc(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be6153",
   "metadata": {},
   "source": [
    "### Remove Hashsign \"#\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d333c3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Our Deeds are the Reason of this earthquake Ma...\n",
       "1               Forest fire near La Ronge Sask. Canada\n",
       "2    All residents asked to 'shelter in place' are ...\n",
       "3    13,000 people receive wildfires evacuation ord...\n",
       "4    Just got sent this photo from Ruby Alaska as s...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text'] = df_train.loc[:,'text'].apply(lambda x : re.sub(r'#','',x))\n",
    "df_train['text'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5024c9",
   "metadata": {},
   "source": [
    "### Remove Hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb542aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train.loc[:,'text'].apply(lambda x : re.sub(r'https?:\\/\\/.*[\\r\\n]*','',x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b065d8",
   "metadata": {},
   "source": [
    "### Remove old style retweet text \"RT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "529ef663",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text'] = df_train.loc[:,'text'].apply(lambda x : re.sub(r'^RT[\\s]+', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15756b3c",
   "metadata": {},
   "source": [
    "### Tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b112ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
    "df_train['text'] = df_train.loc[:,'text'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aeccf43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [our, deeds, are, the, reason, of, this, earth...\n",
       "1     [forest, fire, near, la, ronge, sask, ., canada]\n",
       "2    [all, residents, asked, to, ', shelter, in, pl...\n",
       "3    [13,000, people, receive, wildfires, evacuatio...\n",
       "4    [just, got, sent, this, photo, from, ruby, ala...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e898369c",
   "metadata": {},
   "source": [
    "### Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf8315d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ZASHANK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d04bf76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6664d442",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [deeds, reason, earthquake, may, allah, forgiv...\n",
       "1     [forest, fire, near, la, ronge, sask, ., canada]\n",
       "2    [residents, asked, ', shelter, place, ', notif...\n",
       "3    [13,000, people, receive, wildfires, evacuatio...\n",
       "4    [got, sent, photo, ruby, alaska, smoke, wildfi...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_eng = stopwords.words('english')\n",
    "def remove_st_wrds(text):\n",
    "    clean = []\n",
    "    for char in text:\n",
    "        if char not in stopwords_eng:\n",
    "            clean.append(char)\n",
    "    return clean\n",
    "\n",
    "df_train['text'] = df_train.loc[:,'text'].apply(lambda x: remove_st_wrds(x))\n",
    "df_train['text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505505e4",
   "metadata": {},
   "source": [
    "### Stemming "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ec41063",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, ., canada]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[resid, ask, ', shelter, place, ', notifi, off...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[13,000, peopl, receiv, wildfir, evacu, order,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1   4     NaN      NaN    [forest, fire, near, la, rong, sask, ., canada]   \n",
       "2   5     NaN      NaN  [resid, ask, ', shelter, place, ', notifi, off...   \n",
       "3   6     NaN      NaN  [13,000, peopl, receiv, wildfir, evacu, order,...   \n",
       "4   7     NaN      NaN  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate stemming class\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stemming_func(text):\n",
    "    stemmed = []\n",
    "    for char in text:\n",
    "        stem_word = stemmer.stem(char)\n",
    "        stemmed.append(stem_word)\n",
    "    return stemmed\n",
    "\n",
    "df_train['text'] = df_train.loc[:,\"text\"].apply(lambda x: stemming_func(x))\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d2766b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing ID column from train and test data\n",
    "train_id = df_train['id']\n",
    "test_id = df_test['id']\n",
    "\n",
    "# Dropping id column\n",
    "df_train.drop(['id'],axis = 1,inplace = True)\n",
    "df_test.drop(['id'],axis = 1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1abb609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "keyword       26\n",
       "location    1105\n",
       "text           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now treating test data\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9afb1baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test[['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fcb052a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                 Just happened a terrible car crash\n",
       "1  Heard about #earthquake is different cities, s...\n",
       "2  there is a forest fire at spot pond, geese are...\n",
       "3           Apocalypse lighting. #Spokane #wildfires\n",
       "4      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2292f9a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      just happened a terrible car crash\n",
       "1       heard about #earthquake is different cities, s...\n",
       "2       there is a forest fire at spot pond, geese are...\n",
       "3                apocalypse lighting. #spokane #wildfires\n",
       "4           typhoon soudelor kills 28 in china and taiwan\n",
       "                              ...                        \n",
       "3258    earthquake safety los angeles ûò safety faste...\n",
       "3259    storm in ri worse than last hurricane. my city...\n",
       "3260    green line derailment in chicago http://t.co/u...\n",
       "3261    meg issues hazardous weather outlook (hwo) htt...\n",
       "3262    #cityofcalgary has activated its municipal eme...\n",
       "Name: text, Length: 3263, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['text'] = df_test.loc[:,'text'].apply(lambda x : x.lower())\n",
    "df_test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2388723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      just happened a terrible car crash\n",
       "1       heard about #earthquake is different cities, s...\n",
       "2       there is a forest fire at spot pond, geese are...\n",
       "3                apocalypse lighting. #spokane #wildfires\n",
       "4           typhoon soudelor kills 28 in china and taiwan\n",
       "                              ...                        \n",
       "3258    earthquake safety los angeles ûò safety faste...\n",
       "3259    storm in ri worse than last hurricane. my city...\n",
       "3260    green line derailment in chicago http://t.co/u...\n",
       "3261    meg issues hazardous weather outlook (hwo) htt...\n",
       "3262    #cityofcalgary has activated its municipal eme...\n",
       "Name: text, Length: 3263, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string,time\n",
    "exclude = string.punctuation\n",
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text = text.replace(char,'')\n",
    "        return text\n",
    "df_test['text'] = df_test.loc[:,'text'].apply(lambda x : remove_punc(x))\n",
    "df_test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "36000a26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                   just happened a terrible car crash\n",
       "1    heard about earthquake is different cities, st...\n",
       "2    there is a forest fire at spot pond, geese are...\n",
       "3               apocalypse lighting. spokane wildfires\n",
       "4        typhoon soudelor kills 28 in china and taiwan\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['text'] = df_test.loc[:,'text'].apply(lambda x : re.sub(r'#','',x))\n",
    "df_test['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4b0c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text'] = df_test.loc[:,'text'].apply(lambda x : re.sub(r'https?:\\/\\/.*[\\r\\n]*','',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf7ed960",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['text'] = df_test.loc[:,'text'].apply(lambda x : re.sub(r'^RT[\\s]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a09b97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
    "df_test['text'] = df_test.loc[:,'text'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b381eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                     [happened, terrible, car, crash]\n",
       "1    [heard, earthquake, different, cities, ,, stay...\n",
       "2    [forest, fire, spot, pond, ,, geese, fleeing, ...\n",
       "3        [apocalypse, lighting, ., spokane, wildfires]\n",
       "4        [typhoon, soudelor, kills, 28, china, taiwan]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_eng = stopwords.words('english')\n",
    "def remove_st_wrds(text):\n",
    "    clean = []\n",
    "    for char in text:\n",
    "        if char not in stopwords_eng:\n",
    "            clean.append(char)\n",
    "    return clean\n",
    "\n",
    "df_test['text'] = df_test.loc[:,'text'].apply(lambda x: remove_st_wrds(x))\n",
    "df_test['text'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76eec5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[happen, terribl, car, crash]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[heard, earthquak, differ, citi, ,, stay, safe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[forest, fire, spot, pond, ,, gees, flee, acro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[apocalyps, light, ., spokan, wildfir]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[typhoon, soudelor, kill, 28, china, taiwan]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0                      [happen, terribl, car, crash]\n",
       "1  [heard, earthquak, differ, citi, ,, stay, safe...\n",
       "2  [forest, fire, spot, pond, ,, gees, flee, acro...\n",
       "3             [apocalyps, light, ., spokan, wildfir]\n",
       "4       [typhoon, soudelor, kill, 28, china, taiwan]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate stemming class\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stemming_func(text):\n",
    "    stemmed = []\n",
    "    for char in text:\n",
    "        stem_word = stemmer.stem(char)\n",
    "        stemmed.append(stem_word)\n",
    "    return stemmed\n",
    "\n",
    "df_test['text'] = df_test.loc[:,'text'].apply(lambda x: stemming_func(x))\n",
    "df_test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac10b97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[deed, reason, earthquak, may, allah, forgiv, us]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[forest, fire, near, la, rong, sask, ., canada]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[resid, ask, ', shelter, place, ', notifi, off...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[13,000, peopl, receiv, wildfir, evacu, order,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[got, sent, photo, rubi, alaska, smoke, wildfi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[two, giant, crane, hold, bridg, collaps, near...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[control, wild, fire, california, even, northe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[m1, ., 94, [, 01:04, utc, ], ?, 5km, volcano,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[polic, investig, e-bik, collid, car, littl, p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[latest, :, home, raze, northern, california, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     keyword location                                               text  \\\n",
       "0        NaN      NaN  [deed, reason, earthquak, may, allah, forgiv, us]   \n",
       "1        NaN      NaN    [forest, fire, near, la, rong, sask, ., canada]   \n",
       "2        NaN      NaN  [resid, ask, ', shelter, place, ', notifi, off...   \n",
       "3        NaN      NaN  [13,000, peopl, receiv, wildfir, evacu, order,...   \n",
       "4        NaN      NaN  [got, sent, photo, rubi, alaska, smoke, wildfi...   \n",
       "...      ...      ...                                                ...   \n",
       "7608     NaN      NaN  [two, giant, crane, hold, bridg, collaps, near...   \n",
       "7609     NaN      NaN  [control, wild, fire, california, even, northe...   \n",
       "7610     NaN      NaN  [m1, ., 94, [, 01:04, utc, ], ?, 5km, volcano,...   \n",
       "7611     NaN      NaN  [polic, investig, e-bik, collid, car, littl, p...   \n",
       "7612     NaN      NaN  [latest, :, home, raze, northern, california, ...   \n",
       "\n",
       "      target  \n",
       "0          1  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "...      ...  \n",
       "7608       1  \n",
       "7609       1  \n",
       "7610       1  \n",
       "7611       1  \n",
       "7612       1  \n",
       "\n",
       "[7613 rows x 4 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cb6cbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deed reason earthquak may allah forgiv us</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la rong sask . canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>resid ask ' shelter place ' notifi offic . eva...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 peopl receiv wildfir evacu order califo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>got sent photo rubi alaska smoke wildfir pour ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  keyword location                                               text  target\n",
       "0     NaN      NaN          deed reason earthquak may allah forgiv us       1\n",
       "1     NaN      NaN             forest fire near la rong sask . canada       1\n",
       "2     NaN      NaN  resid ask ' shelter place ' notifi offic . eva...       1\n",
       "3     NaN      NaN  13,000 peopl receiv wildfir evacu order califo...       1\n",
       "4     NaN      NaN  got sent photo rubi alaska smoke wildfir pour ...       1"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['text'] = df_train.loc[:,'text'].apply(lambda x: \" \".join(x))\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a2a55bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(df_train.loc[:,'text'],columns = ['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c868cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cfbc9210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8105b09f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       ...,\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.reshape(df_train.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b5fce22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= np.array(y, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b2b1cd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a603fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "81b1f40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = cv.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e69ff345",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c9001c0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 7613]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [54]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2417\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2417\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2419\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2420\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2421\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2422\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:378\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    377\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 378\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    335\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 7613]"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b447d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb1228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
